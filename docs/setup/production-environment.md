# ğŸ­ Kubernetesæœ¬ç•ªç’°å¢ƒæ§‹ç¯‰ã‚¬ã‚¤ãƒ‰

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€AWS ECSé‹ç”¨çµŒé¨“è€…å‘ã‘ã«ã€Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®æœ¬ç•ªç’°å¢ƒæ§‹ç¯‰ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚é«˜å¯ç”¨æ€§ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€ç›£è¦–ã€ç½å®³å¾©æ—§ãªã©ã®ä¼æ¥­ãƒ¬ãƒ™ãƒ«ã®è¦ä»¶ã‚’æº€ãŸã™æ§‹æˆã‚’å­¦ç¿’ã—ã¾ã™ã€‚

## ğŸ“‹ ç›®æ¬¡

1. [AWS ECSã¨ã®å¯¾å¿œé–¢ä¿‚](#aws-ecsã¨ã®å¯¾å¿œé–¢ä¿‚)
2. [æœ¬ç•ªç’°å¢ƒè¦ä»¶](#æœ¬ç•ªç’°å¢ƒè¦ä»¶)
3. [ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](#ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
4. [AWS EKSæ§‹ç¯‰](#aws-eksæ§‹ç¯‰)
5. [ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è¨­å®š](#ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è¨­å®š)
6. [ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š](#ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š)
7. [ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š](#ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š)
8. [ç›£è¦–ãƒ»ãƒ­ã‚°è¨­å®š](#ç›£è¦–ãƒ­ã‚°è¨­å®š)
9. [ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»ç½å®³å¾©æ—§](#ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç½å®³å¾©æ—§)
10. [CI/CDçµ±åˆ](#cicdçµ±åˆ)
11. [å®Ÿè·µæ¼”ç¿’](#å®Ÿè·µæ¼”ç¿’)

## ğŸ”„ AWS ECSã¨ã®å¯¾å¿œé–¢ä¿‚

### ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ãƒãƒƒãƒ”ãƒ³ã‚°

| AWS ECSç’°å¢ƒ | Kubernetesæœ¬ç•ªç’°å¢ƒ | èª¬æ˜ |
|-------------|-------------------|------|
| **ECS Cluster** | **EKS Cluster** | ã‚³ãƒ³ãƒ†ãƒŠã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŸºç›¤ |
| **EC2 Auto Scaling** | **Cluster Autoscaler** | ãƒãƒ¼ãƒ‰ã®è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° |
| **ALB/NLB** | **AWS Load Balancer Controller** | å¤–éƒ¨ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚° |
| **CloudWatch** | **Prometheus + Grafana** | ç›£è¦–ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›† |
| **ECS Service Discovery** | **CoreDNS + Service** | ã‚µãƒ¼ãƒ“ã‚¹é–“é€šä¿¡ãƒ»åå‰è§£æ±º |
| **IAM Roles for Tasks** | **IRSA (IAM Roles for Service Accounts)** | Podå˜ä½ã®IAMæ¨©é™ |
| **CloudFormation** | **Terraform + Helm** | ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ãƒ‰ç®¡ç† |
| **CodePipeline** | **ArgoCD + GitLab CI** | CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ |

### ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£æ¯”è¼ƒ

```yaml
# AWS ECS: Service Auto Scaling
ECS Service:
  DesiredCount: 3
  MinimumHealthyPercent: 100
  MaximumPercent: 200
  AutoScaling:
    TargetCPUUtilization: 70%
    MinCapacity: 2
    MaxCapacity: 10
```

```yaml
# Kubernetes: HPA + VPA + Cluster Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

## ğŸ¯ æœ¬ç•ªç’°å¢ƒè¦ä»¶

### 1. å¯ç”¨æ€§è¦ä»¶

- **SLA**: 99.9%ä»¥ä¸Šï¼ˆæœˆé–“ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ  < 43.8åˆ†ï¼‰
- **RTO**: 15åˆ†ä»¥å†…ï¼ˆå¾©æ—§æ™‚é–“ç›®æ¨™ï¼‰
- **RPO**: 1æ™‚é–“ä»¥å†…ï¼ˆãƒ‡ãƒ¼ã‚¿æå¤±è¨±å®¹æ™‚é–“ï¼‰
- **ãƒãƒ«ãƒAZ**: æœ€ä½3ã¤ã®AZã«åˆ†æ•£é…ç½®

### 2. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦ä»¶

- **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†é›¢**: ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚µãƒ–ãƒãƒƒãƒˆé…ç½®
- **æš—å·åŒ–**: ä¿å­˜æ™‚ãƒ»è»¢é€æ™‚ãƒ‡ãƒ¼ã‚¿æš—å·åŒ–
- **ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡**: RBAC + Pod Security Policy
- **è„†å¼±æ€§ç®¡ç†**: ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚¹ã‚­ãƒ£ãƒ³ + å®šæœŸæ›´æ–°

### 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶

- **ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“**: 95%ile < 200ms
- **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: 1,000 RPSä»¥ä¸Š
- **ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡**: CPUä½¿ç”¨ç‡ < 70%

### 4. é‹ç”¨è¦ä»¶

- **ç›£è¦–**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆ
- **ãƒ­ã‚°**: é›†ç´„ãƒ»æ¤œç´¢ãƒ»ä¿å­˜ï¼ˆ90æ—¥é–“ï¼‰
- **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**: æ—¥æ¬¡ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»ãƒ†ã‚¹ãƒˆå¾©æ—§
- **æ›´æ–°**: ã‚¼ãƒ­ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ  ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

## ğŸ— ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### ãƒãƒ«ãƒAZé«˜å¯ç”¨æ€§æ§‹æˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AWS EKS Cluster                         â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚    AZ-1a    â”‚  â”‚    AZ-1b    â”‚  â”‚    AZ-1c    â”‚        â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚        â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚        â”‚
â”‚  â”‚ â”‚ Control â”‚ â”‚  â”‚ â”‚ Control â”‚ â”‚  â”‚ â”‚ Control â”‚ â”‚        â”‚
â”‚  â”‚ â”‚ Plane   â”‚ â”‚  â”‚ â”‚ Plane   â”‚ â”‚  â”‚ â”‚ Plane   â”‚ â”‚        â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚        â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚        â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚        â”‚
â”‚  â”‚ â”‚ Worker  â”‚ â”‚  â”‚ â”‚ Worker  â”‚ â”‚  â”‚ â”‚ Worker  â”‚ â”‚        â”‚
â”‚  â”‚ â”‚ Node    â”‚ â”‚  â”‚ â”‚ Node    â”‚ â”‚  â”‚ â”‚ Node    â”‚ â”‚        â”‚
â”‚  â”‚ â”‚ Group   â”‚ â”‚  â”‚ â”‚ Group   â”‚ â”‚  â”‚ â”‚ Group   â”‚ â”‚        â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚        â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚        â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚        â”‚
â”‚  â”‚ â”‚   EBS   â”‚ â”‚  â”‚ â”‚   EBS   â”‚ â”‚  â”‚ â”‚   EBS   â”‚ â”‚        â”‚
â”‚  â”‚ â”‚ Volume  â”‚ â”‚  â”‚ â”‚ Volume  â”‚ â”‚  â”‚ â”‚ Volume  â”‚ â”‚        â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  External DNS   â”‚
                    â”‚   Route 53      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Load Balancer  â”‚
                    â”‚   ALB / NLB     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—è¨­è¨ˆ

```yaml
# æœ¬ç•ªç”¨ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—è¨­å®š
NodeGroups:
  # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ãƒãƒ¼ãƒ‰
  application-nodes:
    instanceTypes: [m5.large, m5.xlarge]
    minSize: 3
    maxSize: 20
    desiredCapacity: 6
    taints:
    - key: workload
      value: application
      effect: NoSchedule
    labels:
      workload: application
      
  # ã‚·ã‚¹ãƒ†ãƒ ç”¨ãƒãƒ¼ãƒ‰
  system-nodes:
    instanceTypes: [t3.medium]
    minSize: 2
    maxSize: 5
    desiredCapacity: 3
    taints:
    - key: workload
      value: system
      effect: NoSchedule
    labels:
      workload: system
      
  # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç”¨ãƒãƒ¼ãƒ‰
  database-nodes:
    instanceTypes: [r5.large, r5.xlarge]
    minSize: 2
    maxSize: 6
    desiredCapacity: 3
    taints:
    - key: workload
      value: database
      effect: NoSchedule
    labels:
      workload: database
```

## â˜ï¸ AWS EKSæ§‹ç¯‰

### 1. Terraformç’°å¢ƒå®šç¾©

```hcl
# terraform/main.tf
terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.23"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.11"
    }
  }
  
  backend "s3" {
    bucket = "my-company-terraform-state"
    key    = "k8s-production/terraform.tfstate"
    region = "us-west-2"
    
    dynamodb_table = "terraform-state-lock"
    encrypt        = true
  }
}

# VPCè¨­å®š
module "vpc" {
  source = "terraform-aws-modules/vpc/aws"
  version = "~> 5.0"

  name = "k8s-production-vpc"
  cidr = "10.0.0.0/16"

  azs             = ["us-west-2a", "us-west-2b", "us-west-2c"]
  private_subnets = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
  public_subnets  = ["10.0.101.0/24", "10.0.102.0/24", "10.0.103.0/24"]

  enable_nat_gateway = true
  enable_vpn_gateway = false
  enable_dns_hostnames = true
  enable_dns_support = true

  public_subnet_tags = {
    "kubernetes.io/cluster/${local.cluster_name}" = "shared"
    "kubernetes.io/role/elb" = "1"
  }

  private_subnet_tags = {
    "kubernetes.io/cluster/${local.cluster_name}" = "shared"
    "kubernetes.io/role/internal-elb" = "1"
  }

  tags = {
    Environment = "production"
    Project     = "k8s-migration"
  }
}

# EKSã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
module "eks" {
  source = "terraform-aws-modules/eks/aws"
  version = "~> 19.0"

  cluster_name    = local.cluster_name
  cluster_version = "1.28"

  vpc_id                         = module.vpc.vpc_id
  subnet_ids                     = module.vpc.private_subnets
  cluster_endpoint_public_access = true
  cluster_endpoint_public_access_cidrs = ["203.0.113.0/24"]  # ä¼šç¤¾IPãƒ¬ãƒ³ã‚¸

  cluster_addons = {
    coredns = {
      most_recent = true
    }
    kube-proxy = {
      most_recent = true
    }
    vpc-cni = {
      most_recent = true
    }
    aws-ebs-csi-driver = {
      most_recent = true
    }
  }

  # ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—
  eks_managed_node_groups = {
    application = {
      name = "application-nodes"
      
      instance_types = ["m5.large"]
      min_size     = 3
      max_size     = 20
      desired_size = 6

      pre_bootstrap_user_data = <<-EOT
        #!/bin/bash
        /etc/eks/bootstrap.sh ${local.cluster_name}
      EOT

      vpc_security_group_ids = [aws_security_group.worker_nodes.id]
      
      labels = {
        workload = "application"
      }
      
      taints = {
        application = {
          key    = "workload"
          value  = "application"
          effect = "NO_SCHEDULE"
        }
      }
    }
    
    system = {
      name = "system-nodes"
      
      instance_types = ["t3.medium"]
      min_size     = 2
      max_size     = 5
      desired_size = 3

      labels = {
        workload = "system"
      }
      
      taints = {
        system = {
          key    = "workload"
          value  = "system"
          effect = "NO_SCHEDULE"
        }
      }
    }
  }

  tags = {
    Environment = "production"
    Project     = "k8s-migration"
  }
}

locals {
  cluster_name = "k8s-production"
}
```

### 2. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚°ãƒ«ãƒ¼ãƒ—è¨­å®š

```hcl
# terraform/security-groups.tf
resource "aws_security_group" "worker_nodes" {
  name_prefix = "${local.cluster_name}-worker-nodes"
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port = 22
    to_port   = 22
    protocol  = "tcp"
    cidr_blocks = ["10.0.0.0/16"]  # VPCå†…ã‹ã‚‰ã®SSHã‚¢ã‚¯ã‚»ã‚¹
  }

  ingress {
    from_port = 80
    to_port   = 80
    protocol  = "tcp"
    cidr_blocks = ["10.0.0.0/16"]
  }

  ingress {
    from_port = 443
    to_port   = 443
    protocol  = "tcp"
    cidr_blocks = ["10.0.0.0/16"]
  }

  ingress {
    from_port = 30000
    to_port   = 32767
    protocol  = "tcp"
    cidr_blocks = ["10.0.0.0/16"]  # NodePortãƒ¬ãƒ³ã‚¸
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "${local.cluster_name}-worker-nodes"
    "kubernetes.io/cluster/${local.cluster_name}" = "owned"
  }
}

resource "aws_security_group" "rds" {
  name_prefix = "${local.cluster_name}-rds"
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port       = 5432
    to_port         = 5432
    protocol        = "tcp"
    security_groups = [aws_security_group.worker_nodes.id]
  }

  tags = {
    Name = "${local.cluster_name}-rds"
  }
}
```

### 3. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ§‹ç¯‰å®Ÿè¡Œ

```bash
# 1. TerraformåˆæœŸåŒ–
cd terraform/
terraform init

# 2. è¨­å®šå€¤ç¢ºèª
terraform plan -var-file="production.tfvars"

# 3. ãƒªã‚½ãƒ¼ã‚¹ä½œæˆ
terraform apply -var-file="production.tfvars"

# 4. kubectlè¨­å®š
aws eks update-kubeconfig --region us-west-2 --name k8s-production

# 5. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç¢ºèª
kubectl get nodes
kubectl get namespaces
```

## ğŸ’¾ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è¨­å®š

### 1. EBS CSI Driverè¨­å®š

```yaml
# storage-classes.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gp3-encrypted
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  fsType: ext4
  encrypted: "true"
  iops: "3000"
  throughput: "125"
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Delete
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gp3-retain
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  fsType: ext4
  encrypted: "true"
  iops: "3000"
  throughput: "125"
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Retain  # æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ç”¨
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: io2-high-performance
provisioner: ebs.csi.aws.com
parameters:
  type: io2
  fsType: ext4
  encrypted: "true"
  iops: "10000"
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Retain
```

### 2. æ°¸ç¶šãƒœãƒªãƒ¥ãƒ¼ãƒ æˆ¦ç•¥

```yaml
# database-pvc.yaml - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç”¨PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-data
  namespace: production
  labels:
    app: postgres
    tier: database
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: gp3-retain
  resources:
    requests:
      storage: 100Gi
---
# shared-storage-pvc.yaml - å…±æœ‰ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç”¨PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-storage
  namespace: production
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: efs-storage
  resources:
    requests:
      storage: 1Ti
```

### 3. EFSè¨­å®šï¼ˆå…±æœ‰ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼‰

```yaml
# efs-csi-driver.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: efs-storage
provisioner: efs.csi.aws.com
parameters:
  provisioningMode: efs-ap
  fileSystemId: fs-0abcd1234567890ef  # äº‹å‰ä½œæˆæ¸ˆã¿EFS ID
  directoryPerms: "0755"
  gidRangeStart: "1000"
  gidRangeEnd: "2000"
  basePath: "/k8s-production"
```

## ğŸŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š

### 1. AWS Load Balancer Controller

```bash
# 1. IAMãƒ­ãƒ¼ãƒ«ä½œæˆ
eksctl create iamserviceaccount \
  --cluster=k8s-production \
  --namespace=kube-system \
  --name=aws-load-balancer-controller \
  --attach-policy-arn=arn:aws:iam::ACCOUNT-ID:policy/AWSLoadBalancerControllerIAMPolicy \
  --override-existing-serviceaccounts \
  --approve

# 2. Helm ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
helm repo add eks https://aws.github.io/eks-charts
helm repo update

helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=k8s-production \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller
```

### 2. æœ¬ç•ªç”¨Ingressè¨­å®š

```yaml
# production-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: production-ingress
  namespace: production
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS": 443}]'
    alb.ingress.kubernetes.io/ssl-redirect: '443'
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-west-2:ACCOUNT-ID:certificate/CERT-ID
    alb.ingress.kubernetes.io/healthcheck-path: /health
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '30'
    alb.ingress.kubernetes.io/healthy-threshold-count: '2'
    alb.ingress.kubernetes.io/unhealthy-threshold-count: '5'
    alb.ingress.kubernetes.io/load-balancer-attributes: |
      idle_timeout.timeout_seconds=60,
      routing.http2.enabled=true,
      access_logs.s3.enabled=true,
      access_logs.s3.bucket=my-company-alb-logs,
      access_logs.s3.prefix=k8s-production
spec:
  rules:
  - host: api.mycompany.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 8080
  - host: app.mycompany.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80
```

### 3. VPC CNIæœ€é©åŒ–

```yaml
# vpc-cni-configuration.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: amazon-vpc-cni
  namespace: kube-system
data:
  enable-pod-eni: "true"
  enable-prefix-delegation: "true"
  warm-prefix-target: "2"
  warm-ip-target: "3"
  minimum-ip-target: "10"
  enable-custom-network-config: "false"
```

## ğŸ” ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š

### 1. Pod Security Standards

```yaml
# pod-security-policy.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
---
apiVersion: v1
kind: Namespace
metadata:
  name: development
  labels:
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
```

### 2. RBACè¨­å®š

```yaml
# rbac-production.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: application-developer
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps", "secrets"]
  verbs: ["get", "list", "create", "update", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "create", "update", "delete"]
- apiGroups: ["networking.k8s.io"]
  resources: ["ingresses"]
  verbs: ["get", "list", "create", "update", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  namespace: production
  name: application-developers
subjects:
- kind: User
  name: dev-team@mycompany.com
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: application-developer
  apiGroup: rbac.authorization.k8s.io
```

### 3. Network Policyï¼ˆè©³ç´°ç‰ˆï¼‰

```yaml
# network-policies-production.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-default
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
    - protocol: TCP
      port: 53
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-monitoring
  namespace: production
spec:
  podSelector:
    matchLabels:
      monitoring: enabled
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 9090
```

## ğŸ“Š ç›£è¦–ãƒ»ãƒ­ã‚°è¨­å®š

### 1. Prometheus Stack ãƒ‡ãƒ—ãƒ­ã‚¤

```bash
# 1. Helm ãƒªãƒã‚¸ãƒˆãƒªè¿½åŠ 
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# 2. ç›£è¦–åå‰ç©ºé–“ä½œæˆ
kubectl create namespace monitoring

# 3. Prometheus Operator ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
helm install prometheus-stack prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --values prometheus-values.yaml
```

```yaml
# prometheus-values.yaml
grafana:
  enabled: true
  persistence:
    enabled: true
    size: 10Gi
    storageClassName: gp3-encrypted
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: alb
      alb.ingress.kubernetes.io/scheme: internal
    hosts:
    - grafana.internal.mycompany.com

prometheus:
  prometheusSpec:
    retention: 30d
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          storageClassName: gp3-retain
          resources:
            requests:
              storage: 100Gi

alertmanager:
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          storageClassName: gp3-encrypted
          resources:
            requests:
              storage: 10Gi
```

### 2. ãƒ­ã‚°é›†ç´„ï¼ˆFluent Bitï¼‰

```yaml
# fluent-bit-values.yaml
image:
  repository: amazon/aws-for-fluent-bit
  tag: latest

serviceAccount:
  create: true
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT-ID:role/fluent-bit-role

config:
  outputs: |
    [OUTPUT]
        Name cloudwatch_logs
        Match kube.*
        region us-west-2
        log_group_name /aws/eks/k8s-production/cluster
        log_stream_prefix fluentbit-
        auto_create_group true

    [OUTPUT]
        Name cloudwatch_logs
        Match application.*
        region us-west-2
        log_group_name /aws/eks/k8s-production/applications
        log_stream_prefix app-
        auto_create_group true
```

```bash
# Fluent Bit ãƒ‡ãƒ—ãƒ­ã‚¤
helm repo add aws https://aws.github.io/eks-charts
helm install fluent-bit aws/aws-for-fluent-bit \
  --namespace kube-system \
  --values fluent-bit-values.yaml
```

## ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»ç½å®³å¾©æ—§

### 1. Veleroè¨­å®š

```bash
# 1. S3ãƒã‚±ãƒƒãƒˆä½œæˆ
aws s3 mb s3://k8s-production-backups

# 2. IAMãƒãƒªã‚·ãƒ¼ãƒ»ãƒ­ãƒ¼ãƒ«ä½œæˆ
eksctl create iamserviceaccount \
  --cluster=k8s-production \
  --name=velero-server \
  --namespace=velero \
  --attach-policy-arn=arn:aws:iam::ACCOUNT-ID:policy/VeleroBackupPolicy \
  --approve

# 3. Velero ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
helm repo add vmware-tanzu https://vmware-tanzu.github.io/helm-charts
helm install velero vmware-tanzu/velero \
  --namespace velero \
  --create-namespace \
  --values velero-values.yaml
```

```yaml
# velero-values.yaml
configuration:
  provider: aws
  backupStorageLocation:
    bucket: k8s-production-backups
    config:
      region: us-west-2
  volumeSnapshotLocation:
    config:
      region: us-west-2

initContainers:
- name: velero-plugin-for-aws
  image: velero/velero-plugin-for-aws:v1.8.0
  imagePullPolicy: IfNotPresent
  volumeMounts:
  - mountPath: /target
    name: plugins

serviceAccount:
  server:
    create: false
    name: velero-server
```

### 2. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

```yaml
# backup-schedules.yaml
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-backup
  namespace: velero
spec:
  schedule: "0 2 * * *"  # æ¯æ—¥åˆå‰2æ™‚
  template:
    includedNamespaces:
    - production
    - staging
    excludedResources:
    - secrets
    - events
    storageLocation: default
    volumeSnapshotLocations:
    - default
    ttl: 720h  # 30æ—¥ä¿å­˜
---
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: weekly-full-backup
  namespace: velero
spec:
  schedule: "0 1 * * 0"  # æ¯é€±æ—¥æ›œæ—¥åˆå‰1æ™‚
  template:
    includeClusterResources: true
    storageLocation: default
    volumeSnapshotLocations:
    - default
    ttl: 2160h  # 90æ—¥ä¿å­˜
```

### 3. ç½å®³å¾©æ—§æ‰‹é †

```bash
# 1. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å¾©æ—§
terraform apply -var-file="production.tfvars"

# 2. Veleroå¾©æ—§
helm install velero vmware-tanzu/velero \
  --namespace velero \
  --create-namespace \
  --values velero-values.yaml

# 3. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©æ—§
velero restore create --from-backup daily-backup-20231215020000

# 4. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹ç¢ºèª
kubectl get pods -n production
kubectl get pvc -n production
```

## ğŸš€ CI/CDçµ±åˆ

### 1. ArgoCDè¨­å®š

```bash
# 1. ArgoCD ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# 2. ArgoCD Serverè¨­å®š
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'

# 3. åˆæœŸãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å–å¾—
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
```

### 2. GitOps ãƒªãƒã‚¸ãƒˆãƒªæ§‹é€ 

```
k8s-gitops/
â”œâ”€â”€ environments/
â”‚   â”œâ”€â”€ production/
â”‚   â”‚   â”œâ”€â”€ applications/
â”‚   â”‚   â”‚   â”œâ”€â”€ web-app/
â”‚   â”‚   â”‚   â””â”€â”€ api-server/
â”‚   â”‚   â”œâ”€â”€ infrastructure/
â”‚   â”‚   â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â”‚   â””â”€â”€ ingress/
â”‚   â”‚   â””â”€â”€ kustomization.yaml
â”‚   â””â”€â”€ staging/
â”œâ”€â”€ base/
â”‚   â”œâ”€â”€ web-app/
â”‚   â””â”€â”€ api-server/
â””â”€â”€ scripts/
    â”œâ”€â”€ deploy.sh
    â””â”€â”€ rollback.sh
```

### 3. Applicationè¨­å®š

```yaml
# argocd-applications.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: web-app-production
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/mycompany/k8s-gitops
    targetRevision: main
    path: environments/production/applications/web-app
  destination:
    server: https://kubernetes.default.svc
    namespace: production
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
```

## ğŸ”§ å®Ÿè·µæ¼”ç¿’

### æ¼”ç¿’1: æœ¬ç•ªç’°å¢ƒã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ§‹ç¯‰

```bash
# 1. Terraformç’°å¢ƒæº–å‚™
git clone https://github.com/mycompany/k8s-terraform
cd k8s-terraform/environments/production

# 2. å¤‰æ•°ãƒ•ã‚¡ã‚¤ãƒ«ç·¨é›†
cp terraform.tfvars.example terraform.tfvars
# ç’°å¢ƒå›ºæœ‰ã®å€¤ã‚’è¨­å®š

# 3. ã‚¤ãƒ³ãƒ•ãƒ©æ§‹ç¯‰
terraform init
terraform plan
terraform apply

# 4. kubectlè¨­å®š
aws eks update-kubeconfig --region us-west-2 --name k8s-production

# 5. ã‚¢ãƒ‰ã‚ªãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
./scripts/install-addons.sh
```

### æ¼”ç¿’2: ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰

```bash
# 1. ç›£è¦–ã‚¹ã‚¿ãƒƒã‚¯ãƒ‡ãƒ—ãƒ­ã‚¤
helm install prometheus-stack prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --values monitoring/prometheus-values.yaml

# 2. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®š
kubectl apply -f monitoring/dashboards/

# 3. ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š
kubectl apply -f monitoring/alerts/production-alerts.yaml

# 4. å‹•ä½œç¢ºèª
kubectl port-forward -n monitoring svc/prometheus-stack-grafana 3000:80
# http://localhost:3000 ã§ã‚¢ã‚¯ã‚»ã‚¹
```

### æ¼”ç¿’3: ç½å®³å¾©æ—§ãƒ†ã‚¹ãƒˆ

```bash
# 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
velero backup create db-backup --include-namespaces production --selector app=postgres

# 2. æ¨¡æ“¬éšœå®³ç™ºç”Ÿ
kubectl delete namespace production

# 3. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©æ—§
velero restore create db-restore --from-backup db-backup

# 4. å¾©æ—§ç¢ºèª
kubectl get pods -n production
kubectl exec -n production deployment/postgres -- psql -U user -d myapp -c "\dt"
```

## ğŸ“š ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. æœ¬ç•ªé‹ç”¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

#### ãƒ‡ãƒ—ãƒ­ã‚¤å‰ç¢ºèª
- [ ] ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ãŒè¨­å®šæ¸ˆã¿
- [ ] ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ãŒå®Ÿè£…æ¸ˆã¿
- [ ] ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆãŒè¨­å®šæ¸ˆã¿
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒè¨­å®šæ¸ˆã¿
- [ ] ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰‹é †ãŒç¢ºèªæ¸ˆã¿

#### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç¢ºèª
- [ ] Pod Security StandardsãŒé©ç”¨æ¸ˆã¿
- [ ] NetworkPolicyãŒè¨­å®šæ¸ˆã¿
- [ ] RBACæ¨©é™ãŒæœ€å°æ¨©é™
- [ ] SecretãŒæš—å·åŒ–æ¸ˆã¿
- [ ] ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚¹ã‚­ãƒ£ãƒ³ãŒå®Ÿè¡Œæ¸ˆã¿

#### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç¢ºèª
- [ ] ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ãŒé©åˆ‡
- [ ] ãƒãƒ¼ãƒ‰ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒ«ãŒæ©Ÿèƒ½
- [ ] ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°ãŒå‡ç­‰
- [ ] ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒSLAä»¥å†…

### 2. é‹ç”¨è‡ªå‹•åŒ–

```yaml
# chaos-engineering.yaml
apiVersion: litmuschaos.io/v1alpha1
kind: ChaosEngine
metadata:
  name: production-chaos
  namespace: production
spec:
  appinfo:
    appns: production
    applabel: "app=web-app"
  chaosServiceAccount: chaos-sa
  experiments:
  - name: pod-delete
    spec:
      components:
        env:
        - name: TOTAL_CHAOS_DURATION
          value: "30"
        - name: CHAOS_INTERVAL
          value: "10"
        - name: FORCE
          value: "false"
```

---

**AWS ECSç®¡ç†è€…ã¸ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹**: 
æœ¬ç•ªç’°å¢ƒæ§‹ç¯‰ã¯æ®µéšçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒé‡è¦ã§ã™ã€‚ã¾ãšã¯å°è¦æ¨¡ãªç’°å¢ƒã§åŸºæœ¬æ§‹æˆã‚’ç¢ºç«‹ã—ã€ç›£è¦–ãƒ»ãƒ­ã‚°ãƒ»ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®åŸºç›¤ã‚’æ•´ãˆã¦ã‹ã‚‰ã€å¾ã€…ã«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã—ã¦ã„ãã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ECSã§ã®é‹ç”¨çµŒé¨“ã‚’æ´»ã‹ã—ã€Infrastructure as Codeã¨GitOpsã®è€ƒãˆæ–¹ã‚’å–ã‚Šå…¥ã‚Œã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šç¢ºå®Ÿã§å†ç¾æ€§ã®é«˜ã„æœ¬ç•ªç’°å¢ƒã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚
