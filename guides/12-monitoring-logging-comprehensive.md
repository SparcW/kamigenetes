# Kubernetesãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚®ãƒ³ã‚°å®Œå…¨å­¦ç¿’ã‚¬ã‚¤ãƒ‰

## ç›®æ¬¡
1. [å­¦ç¿’æ¦‚è¦ã¨å‰æçŸ¥è­˜](#å­¦ç¿’æ¦‚è¦ã¨å‰æçŸ¥è­˜)
2. [AWS ECSã‹ã‚‰Kubernetesã¸ã®ç§»è¡Œè¦–ç‚¹](#aws-ecsã‹ã‚‰kubernetesã¸ã®ç§»è¡Œè¦–ç‚¹)
3. [ãƒ­ã‚®ãƒ³ã‚°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç†è§£](#ãƒ­ã‚®ãƒ³ã‚°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç†è§£)
4. [åŸºæœ¬çš„ãªãƒ­ã‚°å–å¾—ã¨æ“ä½œ](#åŸºæœ¬çš„ãªãƒ­ã‚°å–å¾—ã¨æ“ä½œ)
5. [ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¬ãƒ™ãƒ«ãƒ­ã‚®ãƒ³ã‚°å®Ÿè£…](#ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¬ãƒ™ãƒ«ãƒ­ã‚®ãƒ³ã‚°å®Ÿè£…)
6. [ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°åŸºç¤](#ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°åŸºç¤)
7. [Prometheus + Grafanaå®Ÿè£…](#prometheus--grafanaå®Ÿè£…)
8. [å®Ÿè·µæ¼”ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ](#å®Ÿè·µæ¼”ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ)
9. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)
10. [æœ¬ç•ªé‹ç”¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹](#æœ¬ç•ªé‹ç”¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹)

---

## å­¦ç¿’æ¦‚è¦ã¨å‰æçŸ¥è­˜

### ğŸ¯ å­¦ç¿’ç›®æ¨™

ã“ã®åŒ…æ‹¬çš„ãªã‚¬ã‚¤ãƒ‰ã§ã¯ã€AWS ECSç®¡ç†è€…ãŒKubernetesã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒ­ã‚®ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Œå…¨ã«ç†è§£ã—ã€å®Ÿè£…ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚

**ç¿’å¾—ã‚¹ã‚­ãƒ«**:
- Kubernetesãƒ­ã‚®ãƒ³ã‚°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å®Œå…¨ç†è§£
- kubectl logsã‚’ä½¿ã£ãŸåŠ¹æœçš„ãªãƒ­ã‚°èª¿æŸ»
- ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¬ãƒ™ãƒ«ãƒ­ã‚®ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰
- Prometheus/Grafanaã«ã‚ˆã‚‹ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°å®Ÿè£…
- æœ¬ç•ªç’°å¢ƒã§ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ‰‹æ³•

### ğŸ“‹ å‰æçŸ¥è­˜ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] Kubernetesã®åŸºæœ¬æ¦‚å¿µï¼ˆPodã€Serviceã€Deploymentï¼‰
- [ ] kubectlåŸºæœ¬æ“ä½œ
- [ ] DockeråŸºæœ¬çŸ¥è­˜
- [ ] AWS ECSã§ã®ãƒ­ã‚®ãƒ³ã‚°çµŒé¨“
- [ ] YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç†è§£

### ğŸ• æ¨å®šå­¦ç¿’æ™‚é–“

- **ç†è«–å­¦ç¿’**: 4-6æ™‚é–“
- **å®Ÿè·µæ¼”ç¿’**: 8-10æ™‚é–“
- **ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Œæˆ**: 6-8æ™‚é–“
- **ç·è¨ˆ**: 18-24æ™‚é–“ï¼ˆ3-4æ—¥é–“ï¼‰

---

## AWS ECSã‹ã‚‰Kubernetesã¸ã®ç§»è¡Œè¦–ç‚¹

### ECSãƒ­ã‚®ãƒ³ã‚°ã¨Kubernetesã®å¯¾æ¯”

#### AWS ECS ã§ã®ãƒ­ã‚®ãƒ³ã‚°
```json
{
  "family": "my-web-app",
  "taskDefinition": {
    "logConfiguration": {
      "logDriver": "awslogs",
      "options": {
        "awslogs-group": "/ecs/my-app",
        "awslogs-region": "ap-northeast-1",
        "awslogs-stream-prefix": "web"
      }
    }
  }
}
```

#### Kubernetes ã§ã®åŒç­‰å®Ÿè£…
```yaml
# ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
apiVersion: v1
kind: ConfigMap
metadata:
  name: logging-config
data:
  container-log-max-size: "10Mi"
  container-log-max-files: "3"
---
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-web-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-web-app
  template:
    metadata:
      labels:
        app: my-web-app
      annotations:
        # ãƒ­ã‚°åé›†ã®è¨­å®š
        fluentd.org/include: "true"
        fluentd.org/parser: "json"
    spec:
      containers:
      - name: web
        image: my-web-app:latest
        # æ§‹é€ åŒ–ãƒ­ã‚°å‡ºåŠ›ã®æ¨å¥¨
        env:
        - name: LOG_FORMAT
          value: "json"
        - name: LOG_LEVEL
          value: "info"
```

### é‡è¦ãªæ¦‚å¿µãƒãƒƒãƒ”ãƒ³ã‚°

| AWS ECS | Kubernetes | èª¬æ˜ |
|---------|------------|------|
| CloudWatch Logs | kubectl logs + ãƒ­ã‚®ãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ | ãƒ­ã‚°åé›†ãƒ»è¡¨ç¤º |
| Log Groups | Namespace + Label | ãƒ­ã‚°ã®è«–ç†åˆ†å‰² |
| Log Streams | Pod/Container | ãƒ­ã‚°ã‚¹ãƒˆãƒªãƒ¼ãƒ  |
| Log Retention | ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š | ãƒ­ã‚°ä¿æŒæœŸé–“ |
| CloudWatch Insights | ãƒ­ã‚°é›†ç´„ã‚·ã‚¹ãƒ†ãƒ  (ELK/EFK) | ãƒ­ã‚°æ¤œç´¢ãƒ»åˆ†æ |

---

## ãƒ­ã‚®ãƒ³ã‚°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç†è§£

### Kubernetesãƒ­ã‚®ãƒ³ã‚°ã®3ã¤ã®ãƒ¬ãƒ™ãƒ«

#### 1. åŸºæœ¬ãƒ­ã‚°ï¼ˆPod/Container ãƒ¬ãƒ™ãƒ«ï¼‰
```yaml
# ãƒ‡ãƒãƒƒã‚°ç”¨ã®åŸºæœ¬çš„ãªPod
apiVersion: v1
kind: Pod
metadata:
  name: logging-demo
spec:
  containers:
  - name: app
    image: busybox
    command: ["/bin/sh", "-c"]
    args:
      - "while true; do
           echo \"$(date): ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\";
           echo \"$(date): ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\" >&2;
           sleep 5;
         done"
```

**ç¢ºèªã‚³ãƒãƒ³ãƒ‰**:
```bash
# æ¨™æº–ãƒ­ã‚°è¡¨ç¤º
kubectl logs logging-demo

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚©ãƒ­ãƒ¼
kubectl logs -f logging-demo

# æœ€æ–°ã®Nè¡Œã®ã¿è¡¨ç¤º
kubectl logs --tail=20 logging-demo

# ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ã
kubectl logs --timestamps logging-demo
```

#### 2. ãƒãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ãƒ­ã‚®ãƒ³ã‚°

```yaml
# ãƒãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubelet-config
data:
  kubelet-config.yaml: |
    # ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
    containerLogMaxSize: 10Mi
    containerLogMaxFiles: 3
    # ãƒ­ã‚°ãƒ‰ãƒ©ã‚¤ãƒãƒ¼è¨­å®š
    logging:
      format: json
```

#### 3. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¬ãƒ™ãƒ«ãƒ­ã‚®ãƒ³ã‚°

```yaml
# Fluentd DaemonSet ã§ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¬ãƒ™ãƒ«ãƒ­ã‚®ãƒ³ã‚°
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-logging
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: fluentd-logging
  template:
    metadata:
      labels:
        name: fluentd-logging
    spec:
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch.logging.svc.cluster.local"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
```

---

## åŸºæœ¬çš„ãªãƒ­ã‚°å–å¾—ã¨æ“ä½œ

### kubectl logs å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼

#### åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•
```bash
# åŸºæœ¬çš„ãªãƒ­ã‚°è¡¨ç¤º
kubectl logs <pod-name>

# è¤‡æ•°ã‚³ãƒ³ãƒ†ãƒŠPodã®å ´åˆ
kubectl logs <pod-name> -c <container-name>

# å‰ã®ã‚³ãƒ³ãƒ†ãƒŠã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ãƒ­ã‚°
kubectl logs <pod-name> --previous

# Deploymentã®å…¨Podãƒ­ã‚°
kubectl logs deployment/<deployment-name>

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
kubectl logs -f deployment/<deployment-name>

# è¤‡æ•°Podã®åŒæ™‚ç›£è¦–
kubectl logs -l app=my-app -f

# ç‰¹å®šæ™‚é–“ä»¥é™ã®ãƒ­ã‚°
kubectl logs <pod-name> --since=1h

# æœ€æ–°Nè¡Œã®ã¿
kubectl logs <pod-name> --tail=100
```

#### å®Ÿè·µçš„ãªãƒ­ã‚°èª¿æŸ»ãƒ‘ã‚¿ãƒ¼ãƒ³

```bash
#!/bin/bash
# ãƒ­ã‚°èª¿æŸ»ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹

# 1. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å…¨ä½“ã®ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°æŠ½å‡º
echo "=== ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°æ¤œç´¢ ==="
kubectl logs -l app=my-web-app --since=1h | grep -i error

# 2. ç‰¹å®šæ™‚é–“å¸¯ã®è² è·çŠ¶æ³ç¢ºèª
echo "=== è² è·çŠ¶æ³ç¢ºèª ==="
kubectl logs -l app=my-web-app --since=10m | grep -E "(request|response|latency)"

# 3. Podå†èµ·å‹•ã®èª¿æŸ»
echo "=== Podå†èµ·å‹•èª¿æŸ» ==="
kubectl get events --field-selector reason=Killing,reason=Created --sort-by='.lastTimestamp'

# 4. ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ã«ã‚ˆã‚‹Podçµ‚äº†ç¢ºèª
echo "=== ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ç¢ºèª ==="
kubectl describe pods -l app=my-web-app | grep -A 5 -B 5 "OOMKilled\|Evicted"
```

### æ§‹é€ åŒ–ãƒ­ã‚°ã®å®Ÿè£…

```javascript
// Node.js ã§ã®æ§‹é€ åŒ–ãƒ­ã‚°å®Ÿè£…ä¾‹
const winston = require('winston');

const logger = winston.createLogger({
  level: process.env.LOG_LEVEL || 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.json()
  ),
  defaultMeta: {
    service: 'my-web-app',
    version: process.env.APP_VERSION || '1.0.0',
    namespace: process.env.NAMESPACE || 'default',
    pod: process.env.HOSTNAME || 'unknown'
  },
  transports: [
    new winston.transports.Console()
  ]
});

// ä½¿ç”¨ä¾‹
logger.info('ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†é–‹å§‹', {
  userId: 12345,
  requestId: 'req-abc123',
  method: 'GET',
  path: '/api/users'
});

logger.error('ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼', {
  error: error.message,
  stack: error.stack,
  database: 'postgresql',
  query: 'SELECT * FROM users'
});
```

---

## ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¬ãƒ™ãƒ«ãƒ­ã‚®ãƒ³ã‚°å®Ÿè£…

### EFK ã‚¹ã‚¿ãƒƒã‚¯ (Elasticsearch + Fluentd + Kibana)

#### 1. Elasticsearch ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ§‹ç¯‰

```yaml
# Elasticsearch StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
  namespace: logging
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      initContainers:
      - name: fix-permissions
        image: busybox
        command: ["sh", "-c", "chown -R 1000:1000 /usr/share/elasticsearch/data"]
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
        ports:
        - containerPort: 9200
          name: rest
        - containerPort: 9300
          name: inter-node
        env:
        - name: cluster.name
          value: "elasticsearch-cluster"
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: discovery.seed_hosts
          value: "elasticsearch-0.elasticsearch,elasticsearch-1.elasticsearch,elasticsearch-2.elasticsearch"
        - name: cluster.initial_master_nodes
          value: "elasticsearch-0,elasticsearch-1,elasticsearch-2"
        - name: ES_JAVA_OPTS
          value: "-Xms2g -Xmx2g"
        - name: xpack.security.enabled
          value: "false"
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        resources:
          requests:
            memory: 3Gi
            cpu: 1000m
          limits:
            memory: 4Gi
            cpu: 2000m
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 50Gi
---
# Elasticsearch Service
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: logging
spec:
  clusterIP: None
  ports:
  - port: 9200
    name: rest
  - port: 9300
    name: inter-node
  selector:
    app: elasticsearch
```

#### 2. Fluentd ãƒ­ã‚°åé›†è¨­å®š

```yaml
# Fluentd ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: logging
data:
  fluent.conf: |
    # Kubernetesãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
    <source>
      @type tail
      @id in_tail_container_logs
      path /var/log/containers/*.log
      pos_file /var/log/containers.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>

    # Kubernetesãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
    <filter kubernetes.**>
      @type kubernetes_metadata
      @id filter_kube_metadata
      skip_labels false
      skip_container_metadata false
      skip_namespace_metadata false
    </filter>

    # æ—¥æœ¬èªå¯¾å¿œã®æ­£è¦åŒ–
    <filter kubernetes.**>
      @type record_transformer
      <record>
        hostname ${hostname}
        cluster_name k8s-learning-cluster
        @timestamp ${time}
      </record>
    </filter>

    # ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°é™¤å¤–
    <filter kubernetes.**>
      @type grep
      <exclude>
        key $.kubernetes.namespace_name
        pattern ^(kube-system|kube-public|logging)$
      </exclude>
    </filter>

    # Elasticsearchå‡ºåŠ›
    <match kubernetes.**>
      @type elasticsearch
      @id out_es
      host elasticsearch.logging.svc.cluster.local
      port 9200
      logstash_format true
      logstash_prefix kubernetes
      logstash_dateformat %Y.%m.%d
      include_tag_key true
      type_name _doc
      tag_key @log_name
      <buffer>
        @type file
        path /var/log/buffer/kubernetes
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval 5s
        retry_forever
        retry_max_interval 30
        chunk_limit_size 2M
        queue_limit_length 8
        overflow_action block
      </buffer>
    </match>
---
# Fluentd DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: logging
spec:
  selector:
    matchLabels:
      name: fluentd
  template:
    metadata:
      labels:
        name: fluentd
    spec:
      serviceAccount: fluentd
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch.logging.svc.cluster.local"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        - name: FLUENT_ELASTICSEARCH_SCHEME
          value: "http"
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: config-volume
          mountPath: /fluentd/etc
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: config-volume
        configMap:
          name: fluentd-config
```

#### 3. Kibana å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

```yaml
# Kibana Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      containers:
      - name: kibana
        image: docker.elastic.co/kibana/kibana:7.17.0
        ports:
        - containerPort: 5601
        env:
        - name: ELASTICSEARCH_HOSTS
          value: "http://elasticsearch.logging.svc.cluster.local:9200"
        - name: I18N_LOCALE
          value: "ja-JP"
        resources:
          requests:
            memory: 1Gi
            cpu: 500m
          limits:
            memory: 2Gi
            cpu: 1000m
---
# Kibana Service
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: logging
spec:
  type: NodePort
  ports:
  - port: 5601
    targetPort: 5601
    nodePort: 30601
  selector:
    app: kibana
```

---

## ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°åŸºç¤

### Kubernetesãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ¡ãƒˆãƒªã‚¯ã‚¹

#### Metrics Server ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```yaml
# Metrics Server
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-server
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: metrics-server
  template:
    metadata:
      labels:
        k8s-app: metrics-server
    spec:
      containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=4443
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --metric-resolution=15s
        - --kubelet-insecure-tls
        image: k8s.gcr.io/metrics-server/metrics-server:v0.6.1
        name: metrics-server
        ports:
        - containerPort: 4443
          name: https
          protocol: TCP
        resources:
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - mountPath: /tmp
          name: tmp-dir
      volumes:
      - emptyDir: {}
        name: tmp-dir
```

#### åŸºæœ¬çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèª

```bash
# ãƒãƒ¼ãƒ‰ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡
kubectl top nodes

# Pod ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡
kubectl top pods --all-namespaces

# ç‰¹å®šNamespaceã®Podä½¿ç”¨é‡
kubectl top pods -n my-app

# ä½¿ç”¨é‡ã®å¤šã„Podã‚’ç‰¹å®š
kubectl top pods --all-namespaces --sort-by=cpu
kubectl top pods --all-namespaces --sort-by=memory

# HPA (Horizontal Pod Autoscaler) è¨­å®š
kubectl autoscale deployment my-web-app --cpu-percent=70 --min=2 --max=10
```

---

## Prometheus + Grafanaå®Ÿè£…

### Prometheus Operator ã«ã‚ˆã‚‹ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æ§‹ç¯‰

#### 1. Prometheus Operator ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```yaml
# Prometheus Custom Resource
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus
  namespace: monitoring
spec:
  replicas: 2
  retention: 30d
  storage:
    volumeClaimTemplate:
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 50Gi
  serviceAccountName: prometheus
  serviceMonitorSelector:
    matchLabels:
      team: platform
  ruleSelector:
    matchLabels:
      team: platform
  resources:
    requests:
      memory: 2Gi
      cpu: 1000m
    limits:
      memory: 4Gi
      cpu: 2000m
  securityContext:
    fsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
---
# ServiceMonitor ã§ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: my-web-app-metrics
  namespace: monitoring
  labels:
    team: platform
spec:
  selector:
    matchLabels:
      app: my-web-app
  endpoints:
  - port: metrics
    path: /metrics
    interval: 30s
```

#### 2. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹éœ²å‡º

```javascript
// Node.js ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹
const express = require('express');
const promClient = require('prom-client');

// ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
promClient.collectDefaultMetrics({ prefix: 'my_app_' });

// ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
const httpRequestDuration = new promClient.Histogram({
  name: 'my_app_http_request_duration_seconds',
  help: 'HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†æ™‚é–“',
  labelNames: ['method', 'route', 'status'],
  buckets: [0.1, 0.5, 1, 2, 5]
});

const httpRequestTotal = new promClient.Counter({
  name: 'my_app_http_requests_total',
  help: 'HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆç·æ•°',
  labelNames: ['method', 'route', 'status']
});

const activeConnections = new promClient.Gauge({
  name: 'my_app_active_connections',
  help: 'ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªæ¥ç¶šæ•°'
});

const app = express();

// ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢
app.use((req, res, next) => {
  const start = Date.now();
  
  res.on('finish', () => {
    const duration = (Date.now() - start) / 1000;
    const route = req.route?.path || req.path;
    
    httpRequestDuration
      .labels(req.method, route, res.statusCode.toString())
      .observe(duration);
    
    httpRequestTotal
      .labels(req.method, route, res.statusCode.toString())
      .inc();
  });
  
  next();
});

// ãƒ¡ãƒˆãƒªã‚¯ã‚¹å…¬é–‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
app.get('/metrics', (req, res) => {
  res.set('Content-Type', promClient.register.contentType);
  res.end(promClient.register.metrics());
});

app.listen(3000, () => {
  console.log('ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹: http://localhost:3000');
});
```

#### 3. Grafana ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®š

```yaml
# Grafana Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:9.1.0
        ports:
        - containerPort: 3000
        env:
        - name: GF_SECURITY_ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              name: grafana-secret
              key: admin-password
        - name: GF_INSTALL_PLUGINS
          value: "grafana-clock-panel,grafana-simple-json-datasource"
        volumeMounts:
        - name: grafana-storage
          mountPath: /var/lib/grafana
        - name: grafana-config
          mountPath: /etc/grafana/provisioning
        resources:
          requests:
            memory: 512Mi
            cpu: 250m
          limits:
            memory: 1Gi
            cpu: 500m
      volumes:
      - name: grafana-storage
        persistentVolumeClaim:
          claimName: grafana-pvc
      - name: grafana-config
        configMap:
          name: grafana-config
---
# Grafana ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-config
  namespace: monitoring
data:
  datasource.yaml: |
    apiVersion: 1
    datasources:
    - name: Prometheus
      type: prometheus
      access: proxy
      url: http://prometheus.monitoring.svc.cluster.local:9090
      isDefault: true
  dashboard.yaml: |
    apiVersion: 1
    providers:
    - name: 'default'
      orgId: 1
      folder: ''
      type: file
      disableDeletion: false
      editable: true
      options:
        path: /var/lib/grafana/dashboards
```

### PrometheusRule ã«ã‚ˆã‚‹ ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: my-app-alerts
  namespace: monitoring
  labels:
    team: platform
spec:
  groups:
  - name: my-app.rules
    rules:
    # é«˜CPUä½¿ç”¨ç‡ã‚¢ãƒ©ãƒ¼ãƒˆ
    - alert: PodHighCPU
      expr: (rate(container_cpu_usage_seconds_total[5m]) * 100) > 80
      for: 2m
      labels:
        severity: warning
        team: platform
      annotations:
        summary: "Pod {{ $labels.pod }} ã®CPUä½¿ç”¨ç‡ãŒé«˜ã„"
        description: "Pod {{ $labels.pod }} ã®CPUä½¿ç”¨ç‡ãŒ {{ $value }}% ã§ã™"
    
    # é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã‚¢ãƒ©ãƒ¼ãƒˆ
    - alert: PodHighMemory
      expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) * 100 > 90
      for: 5m
      labels:
        severity: critical
        team: platform
      annotations:
        summary: "Pod {{ $labels.pod }} ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé«˜ã„"
        description: "Pod {{ $labels.pod }} ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒ {{ $value }}% ã§ã™"
    
    # HTTPã‚¨ãƒ©ãƒ¼ç‡ã‚¢ãƒ©ãƒ¼ãƒˆ
    - alert: HTTPHighErrorRate
      expr: (rate(my_app_http_requests_total{status=~"5.."}[5m]) / rate(my_app_http_requests_total[5m])) * 100 > 5
      for: 3m
      labels:
        severity: warning
        team: platform
      annotations:
        summary: "HTTPã‚¨ãƒ©ãƒ¼ç‡ãŒé«˜ã„"
        description: "éå»5åˆ†é–“ã®HTTPã‚¨ãƒ©ãƒ¼ç‡ãŒ {{ $value }}% ã§ã™"
    
    # Podå†èµ·å‹•ã‚¢ãƒ©ãƒ¼ãƒˆ
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
      for: 5m
      labels:
        severity: critical
        team: platform
      annotations:
        summary: "Pod {{ $labels.pod }} ãŒç¹°ã‚Šè¿”ã—å†èµ·å‹•ã—ã¦ã„ã‚‹"
        description: "Pod {{ $labels.pod }} ãŒ15åˆ†é–“ã§ {{ $value }} å›å†èµ·å‹•ã—ã¾ã—ãŸ"
```

---

## å®Ÿè·µæ¼”ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

### ç·åˆãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚®ãƒ³ã‚°ç’°å¢ƒæ§‹ç¯‰

#### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦
Node.js Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨PostgreSQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãªã‚‹ã‚·ã‚¹ãƒ†ãƒ ã«å¯¾ã—ã¦ã€åŒ…æ‹¬çš„ãªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚®ãƒ³ã‚°ç’°å¢ƒã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

#### 1. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æº–å‚™

```yaml
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åå‰ç©ºé–“
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring-demo
  labels:
    monitoring: enabled
---
# PostgreSQL Secret
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
  namespace: monitoring-demo
type: Opaque
data:
  username: cG9zdGdyZXM=  # postgres
  password: cGFzc3dvcmQxMjM=  # password123
---
# PostgreSQL ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-config
  namespace: monitoring-demo
data:
  postgresql.conf: |
    # ãƒ­ã‚°è¨­å®š
    log_destination = 'stderr'
    logging_collector = on
    log_directory = '/var/log/postgresql'
    log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
    log_statement = 'all'
    log_min_duration_statement = 100
    
    # æ¥ç¶šè¨­å®š
    max_connections = 100
    shared_buffers = 128MB
---
# PostgreSQL StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: monitoring-demo
spec:
  serviceName: postgres
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9187"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: postgres
        image: postgres:13
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_DB
          value: monitoring_demo
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: username
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        volumeMounts:
        - name: postgres-data
          mountPath: /var/lib/postgresql/data
        - name: postgres-config
          mountPath: /etc/postgresql
        resources:
          requests:
            memory: 256Mi
            cpu: 250m
          limits:
            memory: 512Mi
            cpu: 500m
      
      # PostgreSQL Exporterï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ï¼‰
      - name: postgres-exporter
        image: prometheuscommunity/postgres-exporter:v0.10.1
        ports:
        - containerPort: 9187
        env:
        - name: DATA_SOURCE_NAME
          value: "postgresql://postgres:password123@localhost:5432/monitoring_demo?sslmode=disable"
        resources:
          requests:
            memory: 64Mi
            cpu: 50m
          limits:
            memory: 128Mi
            cpu: 100m
      
      volumes:
      - name: postgres-config
        configMap:
          name: postgres-config
  volumeClaimTemplates:
  - metadata:
      name: postgres-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
```

#### 2. è©³ç´°ãª Web ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

```yaml
# Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: webapp-config
  namespace: monitoring-demo
data:
  app.env: |
    NODE_ENV=production
    PORT=3000
    METRICS_PORT=9090
    LOG_LEVEL=info
    DATABASE_URL=postgresql://postgres:password123@postgres:5432/monitoring_demo
---
# Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  namespace: monitoring-demo
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: webapp
        image: node:16-alpine
        ports:
        - containerPort: 3000
          name: http
        - containerPort: 9090
          name: metrics
        env:
        - name: NODE_ENV
          value: production
        - name: PORT
          value: "3000"
        - name: METRICS_PORT
          value: "9090"
        - name: DATABASE_URL
          value: "postgresql://postgres:password123@postgres:5432/monitoring_demo"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: app-config
          mountPath: /app/.env
          subPath: app.env
        - name: app-code
          mountPath: /app
        command: ["node", "/app/server.js"]
        resources:
          requests:
            memory: 128Mi
            cpu: 100m
          limits:
            memory: 256Mi
            cpu: 500m
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
      
      volumes:
      - name: app-config
        configMap:
          name: webapp-config
      - name: app-code
        configMap:
          name: webapp-source
---
# Service è¨­å®š
apiVersion: v1
kind: Service
metadata:
  name: webapp
  namespace: monitoring-demo
  labels:
    app: webapp
spec:
  type: NodePort
  ports:
  - port: 3000
    targetPort: 3000
    nodePort: 30100
    name: http
  - port: 9090
    targetPort: 9090
    name: metrics
  selector:
    app: webapp
```

#### 3. ãƒ­ã‚°è§£æã¨ã‚¢ãƒ©ãƒ¼ãƒˆã®å®Ÿè£…

```yaml
# ã‚«ã‚¹ã‚¿ãƒ ãƒ­ã‚°ãƒ‘ãƒ¼ã‚µãƒ¼è¨­å®š
apiVersion: v1
kind: ConfigMap
metadata:
  name: log-parser-config
  namespace: monitoring-demo
data:
  parser.conf: |
    [PARSER]
        Name webapp_json
        Format json
        Time_Key timestamp
        Time_Format %Y-%m-%dT%H:%M:%S.%L%z
        Time_Keep On
        Decode_Field_As escaped_utf8 message
    
    [PARSER]
        Name postgres_log
        Format regex
        Regex ^(?<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}.\d{3} \w+)\s+\[(?<pid>\d+)\]\s+(?<level>\w+):\s+(?<message>.*)$
        Time_Key timestamp
        Time_Format %Y-%m-%d %H:%M:%S.%L %Z
---
# FluentBit ãƒ­ã‚°åé›†è¨­å®š
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: monitoring-demo
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         1
        Log_Level     info
        Daemon        off
        Parsers_File  parsers.conf
    
    [INPUT]
        Name              tail
        Tag               webapp.*
        Path              /var/log/containers/*webapp*.log
        Parser            webapp_json
        DB                /var/log/webapp.db
        Mem_Buf_Limit     50MB
        Skip_Long_Lines   On
        Refresh_Interval  10
    
    [INPUT]
        Name              tail
        Tag               postgres.*
        Path              /var/log/containers/*postgres*.log
        Parser            postgres_log
        DB                /var/log/postgres.db
        Mem_Buf_Limit     50MB
        Skip_Long_Lines   On
        Refresh_Interval  10
    
    [FILTER]
        Name                kubernetes
        Match               *
        Kube_URL            https://kubernetes.default.svc:443
        Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
        Kube_Tag_Prefix     kube.var.log.containers.
        Merge_Log           On
        Keep_Log            Off
        Annotations         Off
        Labels              On
    
    [FILTER]
        Name                grep
        Match               webapp.*
        Regex               level (error|warn|info)
    
    [OUTPUT]
        Name                elasticsearch
        Match               *
        Host                elasticsearch.logging.svc.cluster.local
        Port                9200
        Index               monitoring-demo
        Type                _doc
        Logstash_Format     On
        Logstash_Prefix     monitoring-demo
        Logstash_DateFormat %Y.%m.%d
        Time_Key            @timestamp
        Time_Key_Format     %Y-%m-%dT%H:%M:%S.%L%z
        Include_Tag_Key     On
        Tag_Key             @log_name
        Generate_ID         On
```

---

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ä¸€èˆ¬çš„ãªå•é¡Œã¨è§£æ±ºæ–¹æ³•

#### 1. ãƒ­ã‚°ãŒè¡¨ç¤ºã•ã‚Œãªã„å•é¡Œ

```bash
# è¨ºæ–­æ‰‹é †
echo "=== ãƒ­ã‚°å•é¡Œè¨ºæ–­ ==="

# 1. PodçŠ¶æ…‹ç¢ºèª
kubectl get pods -n my-app
kubectl describe pod <pod-name> -n my-app

# 2. ã‚³ãƒ³ãƒ†ãƒŠãƒ­ã‚°ç¢ºèª
kubectl logs <pod-name> -n my-app
kubectl logs <pod-name> -c <container-name> -n my-app --previous

# 3. ã‚¤ãƒ™ãƒ³ãƒˆç¢ºèª
kubectl get events -n my-app --sort-by='.lastTimestamp'

# 4. kubelet ãƒ­ã‚°ç¢ºèª
journalctl -u kubelet | grep -i error

# 5. Docker/containerd ãƒ­ã‚°ç¢ºèª
journalctl -u docker
# ã¾ãŸã¯
journalctl -u containerd
```

#### 2. ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†å•é¡Œ

```bash
# Prometheus ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç¢ºèª
kubectl port-forward -n monitoring svc/prometheus 9090:9090
# ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:9090/targets ã‚’ç¢ºèª

# ServiceMonitorç¢ºèª
kubectl get servicemonitor -n monitoring
kubectl describe servicemonitor <name> -n monitoring

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç›´æ¥ç¢ºèª
kubectl port-forward <pod-name> 9090:9090
curl http://localhost:9090/metrics
```

#### 3. é«˜è² è·æ™‚ã®ãƒ­ã‚°èª¿æŸ»

```bash
#!/bin/bash
# é«˜è² è·èª¿æŸ»ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

echo "=== é«˜è² è·æ™‚ã®ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹èª¿æŸ» ==="

# ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ä¸Šä½Pod
echo "--- CPUä½¿ç”¨é‡TOP10 ---"
kubectl top pods --all-namespaces --sort-by=cpu | head -10

echo "--- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡TOP10 ---"
kubectl top pods --all-namespaces --sort-by=memory | head -10

# ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°æŠ½å‡º
echo "--- éå»1æ™‚é–“ã®ã‚¨ãƒ©ãƒ¼ãƒ­ã‚° ---"
kubectl logs -l app=webapp --since=1h | grep -i "error\|exception\|fail" | tail -20

# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é–¢é€£ã‚¨ãƒ©ãƒ¼
echo "--- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ ---"
kubectl get events --all-namespaces | grep -i "network\|dns\|timeout"

# ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å•é¡Œ
echo "--- ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å•é¡Œ ---"
kubectl get pvc --all-namespaces | grep -v Bound
kubectl get events | grep -i "volume\|pvc\|storage"
```

#### 4. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œè¨ºæ–­

```yaml
# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨Job
apiVersion: batch/v1
kind: Job
metadata:
  name: performance-test
spec:
  template:
    spec:
      containers:
      - name: load-test
        image: williamyeh/wrk
        command: ["wrk"]
        args: [
          "-t12",
          "-c400",
          "-d30s",
          "--timeout=10s",
          "http://webapp.monitoring-demo.svc.cluster.local:3000/"
        ]
      restartPolicy: Never
```

---

## æœ¬ç•ªé‹ç”¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š

#### 1. RBAC ã«ã‚ˆã‚‹ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡

```yaml
# ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ç”¨ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
apiVersion: v1
kind: ServiceAccount
metadata:
  name: monitoring-user
  namespace: monitoring
---
# ClusterRole å®šç¾©
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: monitoring-reader
rules:
- apiGroups: [""]
  resources: ["nodes", "nodes/metrics", "pods", "services", "endpoints"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["extensions", "apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["monitoring.coreos.com"]
  resources: ["servicemonitors", "prometheusrules"]
  verbs: ["get", "list", "watch"]
---
# ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: monitoring-reader-binding
subjects:
- kind: ServiceAccount
  name: monitoring-user
  namespace: monitoring
roleRef:
  kind: ClusterRole
  name: monitoring-reader
  apiGroup: rbac.authorization.k8s.io
```

#### 2. Network Policy ã«ã‚ˆã‚‹é€šä¿¡åˆ¶å¾¡

```yaml
# ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°é–“é€šä¿¡è¨±å¯
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: monitoring-network-policy
  namespace: monitoring
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 9090  # Prometheus
    - protocol: TCP
      port: 3000  # Grafana
  egress:
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 9090  # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
    - protocol: TCP
      port: 443   # HTTPS
    - protocol: TCP
      port: 53    # DNS
    - protocol: UDP
      port: 53    # DNS
```

### è‡ªå‹•åŒ–ã¨CI/CDçµ±åˆ

#### 1. GitOps ã«ã‚ˆã‚‹ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨­å®šç®¡ç†

```yaml
# ArgoCD Application
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: monitoring-stack
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/your-org/k8s-monitoring
    targetRevision: HEAD
    path: manifests/monitoring
  destination:
    server: https://kubernetes.default.svc
    namespace: monitoring
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
```

#### 2. Helm ã«ã‚ˆã‚‹æ§‹æˆç®¡ç†

```yaml
# values.yaml for monitoring stack
prometheus:
  retention: 30d
  storage:
    size: 50Gi
  resources:
    requests:
      memory: 2Gi
      cpu: 1000m
    limits:
      memory: 4Gi
      cpu: 2000m

grafana:
  adminPassword: changeme
  persistence:
    enabled: true
    size: 10Gi
  plugins:
    - grafana-clock-panel
    - grafana-simple-json-datasource

elasticsearch:
  replicas: 3
  storage:
    size: 100Gi
  javaOpts: "-Xms2g -Xmx2g"

fluentd:
  resources:
    limits:
      memory: 200Mi
    requests:
      cpu: 100m
      memory: 200Mi
```

### é‹ç”¨ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

#### 1. SLI/SLO ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

```json
{
  "dashboard": {
    "title": "ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«ç›£è¦–",
    "panels": [
      {
        "title": "å¯ç”¨æ€§ (99.9% SLO)",
        "type": "singlestat",
        "targets": [
          {
            "expr": "(1 - (rate(my_app_http_requests_total{status=~\"5..\"}[5m]) / rate(my_app_http_requests_total[5m]))) * 100"
          }
        ],
        "thresholds": "99.9,99.5"
      },
      {
        "title": "ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ (P95 < 500ms SLO)",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(my_app_http_request_duration_seconds_bucket[5m])) * 1000"
          }
        ]
      },
      {
        "title": "ã‚¨ãƒ©ãƒ¼ç‡ (< 1% SLO)",
        "type": "graph",
        "targets": [
          {
            "expr": "(rate(my_app_http_requests_total{status=~\"5..\"}[5m]) / rate(my_app_http_requests_total[5m])) * 100"
          }
        ]
      }
    ]
  }
}
```

#### 2. é‹ç”¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

```json
{
  "dashboard": {
    "title": "Kubernetes ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç›£è¦–",
    "panels": [
      {
        "title": "ãƒãƒ¼ãƒ‰ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡",
        "type": "graph",
        "targets": [
          {
            "expr": "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100",
            "legendFormat": "ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ - {{instance}}"
          },
          {
            "expr": "(1 - rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100",
            "legendFormat": "CPUä½¿ç”¨ç‡ - {{instance}}"
          }
        ]
      },
      {
        "title": "Pod çŠ¶æ…‹",
        "type": "table",
        "targets": [
          {
            "expr": "kube_pod_status_phase",
            "format": "table"
          }
        ]
      }
    ]
  }
}
```

### ã¾ã¨ã‚

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€AWS ECSç®¡ç†è€…å‘ã‘ã«Kubernetesã§ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒ­ã‚®ãƒ³ã‚°ã‚’åŒ…æ‹¬çš„ã«å­¦ç¿’ã™ã‚‹å†…å®¹ã‚’ãŠä¼ãˆã—ã¾ã—ãŸã€‚

**ä¸»è¦ãªå­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ**:

1. **æ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**: åŸºæœ¬ã®kubectl logsã‹ã‚‰å§‹ã¾ã‚Šã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¬ãƒ™ãƒ«ã®æœ¬æ ¼çš„ãªã‚·ã‚¹ãƒ†ãƒ ã¾ã§
2. **å®Ÿè·µé‡è¦–**: å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ã¨è¨­å®šä¾‹ã§å³åº§ã«è©¦ã›ã‚‹å†…å®¹
3. **ECSæ¯”è¼ƒ**: æ—¢å­˜ã®ECSçŸ¥è­˜ã‚’æ´»ã‹ã—ãŸç§»è¡Œã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
4. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é…æ…®**: æœ¬ç•ªé‹ç”¨ã«è€ãˆã‚‹è¨­å®šã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
5. **ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**: å®Ÿéš›ã®é‹ç”¨ã§é­é‡ã™ã‚‹å•é¡Œã¸ã®å¯¾å‡¦æ³•

æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€å®Ÿéš›ã®ç’°å¢ƒã§ã“ã‚Œã‚‰ã®è¨­å®šã‚’è©¦ã—ã€è‡ªç¤¾ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«é©ç”¨ã—ã¦ã¿ã¦ãã ã•ã„ã€‚ç¶™ç¶šçš„ãªå­¦ç¿’ã¨æ”¹å–„ã«ã‚ˆã‚Šã€åŠ¹æœçš„ãªKubernetesãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚®ãƒ³ã‚°ç’°å¢ƒã‚’æ§‹ç¯‰ã§ãã‚‹ã§ã—ã‚‡ã†ã€‚

### ğŸš€ å®Ÿè·µçš„ãªå­¦ç¿’ã‚’å§‹ã‚ã‚ˆã†

ç†è«–ã®å­¦ç¿’ãŒå®Œäº†ã—ãŸã‚‰ã€ãœã²å®Ÿéš›ã«æ‰‹ã‚’å‹•ã‹ã—ã¦å­¦ç¿’ã‚’æ·±ã‚ã¦ãã ã•ã„ã€‚

**ãƒãƒ³ã‚ºã‚ªãƒ³æ¼”ç¿’ã®å ´æ‰€**:
```bash
cd hands-on/monitoring-logging/
./scripts/setup.sh
```

**æ®µéšçš„ãªæ¼”ç¿’å†…å®¹**:
- **Phase 1**: kubectl logs ã®å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼ï¼ˆ60-90åˆ†ï¼‰
- **Phase 2**: å®Ÿéš›ã®Webã‚¢ãƒ—ãƒªã§ã®æ§‹é€ åŒ–ãƒ­ã‚°å®Ÿè£…ï¼ˆ90-120åˆ†ï¼‰
- **Phase 3**: EFK ã‚¹ã‚¿ãƒƒã‚¯ã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¬ãƒ™ãƒ«ãƒ­ã‚®ãƒ³ã‚°ï¼ˆ120-150åˆ†ï¼‰
- **Phase 4**: Prometheus + Grafana ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æ§‹ç¯‰ï¼ˆ120-150åˆ†ï¼‰
- **Phase 5**: ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®šã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼ˆ90-120åˆ†ï¼‰

æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€å®Ÿéš›ã®ç’°å¢ƒã§ã“ã‚Œã‚‰ã®è¨­å®šã‚’è©¦ã—ã€è‡ªç¤¾ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«é©ç”¨ã—ã¦ã¿ã¦ãã ã•ã„ã€‚ç¶™ç¶šçš„ãªå­¦ç¿’ã¨æ”¹å–„ã«ã‚ˆã‚Šã€åŠ¹æœçš„ãªKubernetesãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚®ãƒ³ã‚°ç’°å¢ƒã‚’æ§‹ç¯‰ã§ãã‚‹ã§ã—ã‚‡ã†ã€‚
